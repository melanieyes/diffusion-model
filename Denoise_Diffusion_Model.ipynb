{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melanieyes/diffusion-model/blob/main/Denoise_Diffusion_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2\n"
      ],
      "metadata": {
        "id": "A5j61nSAygqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmY3jhTtzQcE",
        "outputId": "7314fb60-18f5-407d-c7ec-2da280a91f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKiNN2J8og-N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
        "    \"\"\"\n",
        "    Retrieved from https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/nn.py#LL90C1-L109C13\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n",
        "    emb = timesteps.type(torch.float32)[:, None] * emb[None, :]\n",
        "    emb = torch.concat([torch.sin(emb), torch.cos(emb)], axis=1)\n",
        "\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.pad(emb, [[0, 0], [0, 1]])\n",
        "\n",
        "    assert emb.shape == (timesteps.shape[0], embedding_dim), f\"{emb.shape}\"\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = (torch.rand(100)*10).long()\n",
        "get_timestep_embedding(t,64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D30wNLsos0g",
        "outputId": "d968452d-05f8-4be4-9dfd-1c12dd71c054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9093,  0.9964,  0.8930,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.6570, -0.8831, -0.6612,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.9894, -0.3330, -0.9564,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        ...,\n",
              "        [ 0.9894, -0.3330, -0.9564,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.6570, -0.8831, -0.6612,  ...,  1.0000,  1.0000,  1.0000],\n",
              "        [ 0.9894, -0.3330, -0.9564,  ...,  1.0000,  1.0000,  1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(nn.Module):\n",
        "\n",
        "    def __init__(self, C):\n",
        "        \"\"\"\n",
        "        :param C (int): number of input and output channels\n",
        "        \"\"\"\n",
        "        super(Downsample, self).__init__()\n",
        "        self.conv = nn.Conv2d(C, C, 3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.conv(x)\n",
        "        assert x.shape == (B, C, H // 2, W // 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VzDwBGdBqRFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(nn.Module):\n",
        "\n",
        "    def __init__(self, C):\n",
        "        \"\"\"\n",
        "        :param C (int): number of input and output channels\n",
        "        \"\"\"\n",
        "        super(Upsample, self).__init__()\n",
        "        self.conv = nn.Conv2d(C, C, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = nn.functional.interpolate(x, size=None, scale_factor=2, mode='nearest')\n",
        "\n",
        "        x = self.conv(x)\n",
        "        assert x.shape == (B, C, H * 2, W * 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AsMizSthrG-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downsample = Downsample(64)\n",
        "img = torch.randn((10, 64, 400, 400))\n",
        "img_down = downsample(img)\n",
        "print(\"Original image: \", img.shape)\n",
        "\n",
        "upsample = Upsample(64)\n",
        "img_up = upsample(img_down)\n",
        "print(\"Upsampling image: \", img_up.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45i72NFWrjE0",
        "outputId": "8cb50f4e-0be9-46ab-e260-4cfc23110d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image:  torch.Size([10, 64, 400, 400])\n",
            "Upsampling image:  torch.Size([10, 64, 400, 400])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Nin(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, scale = 1e-10):\n",
        "        super(Nin, self).__init__()\n",
        "\n",
        "        n = (in_dim + out_dim) / 2\n",
        "        limit = np.sqrt(3 * scale / n)\n",
        "        self.W = torch.nn.Parameter(torch.zeros((in_dim, out_dim), dtype=torch.float32\n",
        "                                               ).uniform_(-limit, limit))\n",
        "        self.b = torch.nn.Parameter(torch.zeros((1, out_dim, 1, 1), dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum('bchw, co->bowh', x, self.W) + self.b"
      ],
      "metadata": {
        "id": "T2pGy9O9ucx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downsample = Downsample(64)\n",
        "img = torch.randn((10, 64, 400, 400))\n",
        "img_down = downsample(img)\n",
        "print(\"Original image: \", img.shape)\n",
        "\n",
        "upsample = Upsample(64)\n",
        "img_up = upsample(img_down)\n",
        "print(\"Upsampling image: \", img_up.shape)\n",
        "\n",
        "nin = Nin(64,128)\n",
        "nin(img).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNzT9yY9uh_C",
        "outputId": "396c0263-126a-4627-abd1-f060765a4aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image:  torch.Size([10, 64, 400, 400])\n",
            "Upsampling image:  torch.Size([10, 64, 400, 400])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 128, 400, 400])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch, dropout_rate=0.1):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)\n",
        "        self.dense = nn.Linear(512, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1)\n",
        "\n",
        "        if not (in_ch == out_ch):\n",
        "            self.nin = Nin(in_ch, out_ch)\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.nonlinearity = torch.nn.SiLU()\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        :param x: (B, C, H, W)\n",
        "        :param temb: (B, dim)\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.nonlinearity(nn.functional.group_norm(x, num_groups=32))\n",
        "        h = self.conv1(x)\n",
        "\n",
        "        # add in timestep embedding\n",
        "        h += self.dense(self.nonlinearity(temb))[:, :, None, None]\n",
        "\n",
        "        h = self.nonlinearity(nn.functional.group_norm(h, num_groups=32))\n",
        "        h = nn.functional.dropout(h, p=self.dropout_rate)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if not (x.shape[1] == h.shape[1]):\n",
        "            x = self.nin(x)\n",
        "\n",
        "        assert x.shape == h.shape\n",
        "        return x + h"
      ],
      "metadata": {
        "id": "GNvsLDpyvLIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = (torch.rand(10)*10).float()\n",
        "t = get_timestep_embedding(t,512)\n",
        "\n",
        "downsample = Downsample(64)\n",
        "img = torch.randn((10, 64, 128, 128))\n",
        "img_down = downsample(img)\n",
        "print(\"Original image: \", img.shape)\n",
        "print(\"Down image: \", img_down.shape)\n",
        "\n",
        "upsample = Upsample(64)\n",
        "img_up = upsample(img_down)\n",
        "print(\"Upsampling image: \", img_up.shape)\n",
        "\n",
        "nin = Nin(64,128)\n",
        "img = nin(img_up)\n",
        "print(\"Nin \", img.shape)\n",
        "\n",
        "resnet = ResNetBlock(128, 128, 0.1)\n",
        "img = resnet(img, t)\n",
        "print(\"ResNet \", img.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55t3yFc0wFg0",
        "outputId": "a59e71c2-686b-4345-e627-1e8971f1efc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image:  torch.Size([10, 64, 128, 128])\n",
            "Down image:  torch.Size([10, 64, 64, 64])\n",
            "Upsampling image:  torch.Size([10, 64, 128, 128])\n",
            "Nin  torch.Size([10, 128, 128, 128])\n",
            "ResNet  torch.Size([10, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNetBlock(128, 128, 0.1)\n",
        "img = resnet(img, t)\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyCx0yjNxZYR",
        "outputId": "68a33141-1101-490e-cd46-87c164527bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 128, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, ch):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        self.Q = Nin(ch, ch)\n",
        "        self.K = Nin(ch, ch)\n",
        "        self.V = Nin(ch, ch)\n",
        "\n",
        "        self.ch = ch\n",
        "\n",
        "        self.nin = Nin(ch, ch, scale=0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        assert C == self.ch\n",
        "\n",
        "        h = nn.functional.group_norm(x, num_groups=32)\n",
        "        q = self.Q(h)\n",
        "        k = self.K(h)\n",
        "        v = self.V(h)\n",
        "\n",
        "        w = torch.einsum('bchw,bcHW->bhwHW', q, k) * (int(C) ** (-0.5)) # [B, H, W, H, W]\n",
        "        w = torch.reshape(w, [B, H, W, H * W])\n",
        "        w = torch.nn.functional.softmax(w, dim=-1)\n",
        "        w = torch.reshape(w, [B, H, W, H, W])\n",
        "\n",
        "        h = torch.einsum('bhwHW,bcHW->bchw', w, v)\n",
        "        h = self.nin(h)\n",
        "\n",
        "        assert h.shape == x.shape\n",
        "        return x + h"
      ],
      "metadata": {
        "id": "_I3x0v5k7P0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = (torch.rand(10)*10).float()\n",
        "t = get_timestep_embedding(t,512)\n",
        "\n",
        "downsample = Downsample(64)\n",
        "img = torch.randn((10, 64, 16, 16))\n",
        "img_down = downsample(img)\n",
        "print(\"Original image: \", img.shape)\n",
        "print(\"Down image: \", img_down.shape)\n",
        "\n",
        "upsample = Upsample(64)\n",
        "img_up = upsample(img_down)\n",
        "print(\"Upsampling image: \", img_up.shape)\n",
        "\n",
        "nin = Nin(64,128)\n",
        "img = nin(img_up)\n",
        "print(\"Nin \", img.shape)\n",
        "\n",
        "resnet = ResNetBlock(128, 128, 0.1)\n",
        "img = resnet(img, t)\n",
        "\n",
        "resnet = ResNetBlock(128, 64, 0.1)\n",
        "img = resnet(img, t)\n",
        "print(\"ResNet \", img.shape)\n",
        "\n",
        "att = AttentionBlock(64)\n",
        "img = att(img)\n",
        "print(\"Attention \", img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pZaqu8e7Ya2",
        "outputId": "a0c04184-8d62-48ef-cadf-5e46fd063d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image:  torch.Size([10, 64, 16, 16])\n",
            "Down image:  torch.Size([10, 64, 8, 8])\n",
            "Upsampling image:  torch.Size([10, 64, 16, 16])\n",
            "Nin  torch.Size([10, 128, 16, 16])\n",
            "ResNet  torch.Size([10, 64, 16, 16])\n",
            "Attention  torch.Size([10, 64, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, ch=128, in_ch=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.ch = ch\n",
        "        self.linear1 = nn.Linear(ch, 4 * ch)\n",
        "        self.linear2 = nn.Linear(4 * ch, 4 * ch)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_ch, ch, 3, stride=1, padding=1)\n",
        "\n",
        "        self.down = nn.ModuleList([ResNetBlock(ch, 1 * ch),\n",
        "                                   ResNetBlock(1 * ch, 1 * ch),\n",
        "                                   Downsample(1 * ch),\n",
        "                                   ResNetBlock(1 * ch, 2 * ch),\n",
        "                                   AttentionBlock(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   AttentionBlock(2 * ch),\n",
        "                                   Downsample(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   Downsample(2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch),\n",
        "                                   ResNetBlock(2 * ch, 2 * ch)])\n",
        "\n",
        "        self.middle = nn.ModuleList([ResNetBlock(2 * ch, 2 * ch),\n",
        "                                     AttentionBlock(2 * ch),\n",
        "                                     ResNetBlock(2 * ch, 2 * ch)])\n",
        "\n",
        "        self.up = nn.ModuleList([ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 ResNetBlock(4 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 ResNetBlock(3 * ch, 2 * ch),\n",
        "                                 AttentionBlock(2 * ch),\n",
        "                                 Upsample(2 * ch),\n",
        "                                 ResNetBlock(3 * ch, ch),\n",
        "                                 ResNetBlock(2 * ch, ch),\n",
        "                                 ResNetBlock(2 * ch, ch)])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(ch, in_ch, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        :param x (torch.Tensor): batch of images [B, C, H, W]\n",
        "        :param t (torch.Tensor): tensor of time steps (torch.long) [B]\n",
        "        \"\"\"\n",
        "\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = torch.nn.functional.silu(self.linear1(temb))\n",
        "        temb = self.linear2(temb)\n",
        "        assert temb.shape == (t.shape[0], self.ch*4)\n",
        "\n",
        "        x1 = self.conv1(x)\n",
        "\n",
        "        # Down\n",
        "        x2 = self.down[0](x1, temb)\n",
        "        x3 = self.down[1](x2, temb)\n",
        "        x4 = self.down[2](x3)\n",
        "        x5 = self.down[3](x4, temb)\n",
        "        x6 = self.down[4](x5)         # Attention\n",
        "        x7 = self.down[5](x6, temb)\n",
        "        x8 = self.down[6](x7)   # Attention\n",
        "        x9 = self.down[7](x8)\n",
        "        x10 = self.down[8](x9, temb)\n",
        "        x11 = self.down[9](x10, temb)\n",
        "        x12 = self.down[10](x11)\n",
        "        x13 = self.down[11](x12, temb)\n",
        "        x14 = self.down[12](x13, temb)\n",
        "\n",
        "        # Middle\n",
        "        x = self.middle[0](x14, temb)\n",
        "        x = self.middle[1](x)\n",
        "        x = self.middle[2](x, temb)\n",
        "\n",
        "        # Up\n",
        "        x = self.up[0](torch.cat((x, x14), dim=1), temb)\n",
        "        x = self.up[1](torch.cat((x, x13), dim=1), temb)\n",
        "        x = self.up[2](torch.cat((x, x12), dim=1), temb)\n",
        "        x = self.up[3](x)\n",
        "        x = self.up[4](torch.cat((x, x11), dim=1), temb)\n",
        "        x = self.up[5](torch.cat((x, x10), dim=1), temb)\n",
        "        x = self.up[6](torch.cat((x, x9), dim=1), temb)\n",
        "        x = self.up[7](x)\n",
        "        x = self.up[8](torch.cat((x, x8), dim=1), temb)\n",
        "        x = self.up[9](x)\n",
        "        x = self.up[10](torch.cat((x, x6), dim=1), temb)\n",
        "        x = self.up[11](x)\n",
        "        x = self.up[12](torch.cat((x, x4), dim=1), temb)\n",
        "        x = self.up[13](x)\n",
        "        x = self.up[14](x)\n",
        "        x = self.up[15](torch.cat((x, x3), dim=1), temb)\n",
        "        x = self.up[16](torch.cat((x, x2), dim=1), temb)\n",
        "        x = self.up[17](torch.cat((x, x1), dim=1), temb)\n",
        "\n",
        "        x = nn.functional.silu(nn.functional.group_norm(x, num_groups=32))\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VF3q69rO8YOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = (torch.rand(10)*10).float()\n",
        "img = torch.randn((10,1,32,32))\n",
        "model = UNet()\n",
        "img = model(img, t)\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s0ZvksE8p6U",
        "outputId": "12df49ce-966e-4f49-8bdd-3ba594ab831c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"# parameters: \", sum([p.numel() for p in model.parameters()])/1e6, \"M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maFaOtY__9ao",
        "outputId": "e50fd887-300b-4ddb-cd2b-393c1b09cd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# parameters:  35.713281 M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel():\n",
        "\n",
        "    def __init__(self, T : int, model : nn.Module, device : str):\n",
        "\n",
        "        self.T = T\n",
        "        self.function_approximator = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "        self.beta = torch.linspace(1e-4, 0.02, T).to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def training(self, batch_size, optimizer):\n",
        "        \"\"\"\n",
        "        Algorithm 1 in Denoising Diffusion Probabilistic Models\n",
        "        \"\"\"\n",
        "\n",
        "        x0 = sample_batch(batch_size, self.device)\n",
        "\n",
        "        t = torch.randint(1, self.T + 1, (batch_size,), device=self.device, dtype=torch.long)\n",
        "\n",
        "        eps = torch.randn_like(x0)\n",
        "\n",
        "        # Take one gradient descent step\n",
        "        alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "        eps_predicted = self.function_approximator(torch.sqrt(\n",
        "            alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * eps, t-1)\n",
        "        loss = nn.functional.mse_loss(eps, eps_predicted)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sampling(self, n_samples=1, image_channels=1, img_size=(32, 32), use_tqdm=True):\n",
        "\n",
        "        x = torch.randn((n_samples, image_channels, img_size[0], img_size[1]),\n",
        "                         device=self.device)\n",
        "\n",
        "        progress_bar = tqdm if use_tqdm else lambda x : x\n",
        "        for t in progress_bar(range(self.T, 0, -1)):\n",
        "            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n",
        "\n",
        "            t = torch.ones(n_samples, dtype=torch.long, device=self.device) * t\n",
        "\n",
        "            beta_t = self.beta[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_t = self.alpha[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "            alpha_bar_t = self.alpha_bar[t-1].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "            mean = 1 / torch.sqrt(alpha_t) * (x - ((1 - alpha_t) / torch.sqrt(\n",
        "                1 - alpha_bar_t)) * self.function_approximator(x, t-1))\n",
        "            sigma = torch.sqrt(beta_t)\n",
        "            x = mean + sigma * z\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2w35rE1XM-la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "trainX = np.float32(trainX) / 255.\n",
        "testX = np.float32(testX) / 255.\n",
        "\n",
        "def sample_batch(batch_size, device):\n",
        "    indices = torch.randperm(trainX.shape[0])[:batch_size]\n",
        "    data = torch.from_numpy(trainX[indices]).unsqueeze(1).to(device)\n",
        "    return torch.nn.functional.interpolate(data, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEEw2OH0MqkF",
        "outputId": "3958ac8a-9f12-457d-840e-aac7eb6c7344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "batch_size = 64\n",
        "model = UNet()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "diffusion_model = DiffusionModel(1000, model, device)"
      ],
      "metadata": {
        "id": "gs7ic4kkM2wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_loss = []\n",
        "for epoch in tqdm(range(40_000)):\n",
        "    loss = diffusion_model.training(batch_size, optimizer)\n",
        "    training_loss.append(loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        plt.plot(training_loss)\n",
        "        plt.savefig('training_loss.png')\n",
        "        plt.close()\n",
        "\n",
        "        plt.plot(training_loss[-1000:])\n",
        "        plt.savefig('training_loss_cropped.png')\n",
        "        plt.close()\n",
        "\n",
        "    if epoch % 5000 == 0:\n",
        "        nb_images=81\n",
        "        samples = diffusion_model.sampling(n_samples=nb_images, use_tqdm=False)\n",
        "        plt.figure(figsize=(17, 17))\n",
        "        for i in range(nb_images):\n",
        "            plt.subplot(9, 9, 1 + i)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')\n",
        "        plt.savefig(f'samples_epoch_{epoch}.png')\n",
        "        plt.close()\n",
        "\n",
        "        torch.save(model.cpu(), f'model_paper2_epoch_{epoch}')\n",
        "        model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDwasHiYNLUl",
        "outputId": "5755bb8b-3551-4b32-963e-62deec73c206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/40000 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_images=81\n",
        "samples = diffusion_model.sampling(n_samples=nb_images, use_tqdm=False)\n",
        "plt.figure(figsize=(17, 17))\n",
        "for i in range(nb_images):\n",
        "    plt.subplot(9, 9, 1 + i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(samples[i].squeeze(0).clip(0, 1).data.cpu().numpy(), cmap='gray')\n",
        "plt.savefig(f'samples_epoch_{epoch}.png')\n",
        "plt.close()\n",
        "\n",
        "torch.save(model.cpu(), f'model_paper2_epoch_{epoch}')"
      ],
      "metadata": {
        "id": "kgWByjN_NYbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}